{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path='dataset.csv', encoding_method='onehot'):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    df = df.drop('Id', axis=1)\n",
    "    df['Vegetation_Type'] = df['Vegetation_Type'].str.replace('Type_', '').astype(int)\n",
    "    \n",
    "    # Split features and target\n",
    "    y = df['Vegetation_Type']\n",
    "    X = df.drop('Vegetation_Type', axis=1)\n",
    "    \n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if encoding_method == 'onehot':\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore', \n",
    "                                drop='first'), categorical_features)\n",
    "        ])\n",
    "        \n",
    "        X_processed = preprocessor.fit_transform(X)\n",
    "        numeric_names = numeric_features.tolist()\n",
    "        categorical_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "        feature_names = numeric_names + categorical_names.tolist()\n",
    "        \n",
    "    elif encoding_method == 'label':\n",
    "        label_encoders = {}\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for cat_feature in categorical_features:\n",
    "            label_encoders[cat_feature] = LabelEncoder()\n",
    "            X_transformed[cat_feature] = label_encoders[cat_feature].fit_transform(X[cat_feature])\n",
    "        \n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('all', StandardScaler(), X_transformed.columns)\n",
    "        ])\n",
    "        \n",
    "        X_processed = preprocessor.fit_transform(X_transformed)\n",
    "        feature_names = X_transformed.columns\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"encoding_method must be either 'onehot' or 'label'\")\n",
    "    \n",
    "    X_processed = pd.DataFrame(X_processed, columns=feature_names)\n",
    "    \n",
    "    print(f\"Data processed using {encoding_method} encoding\")\n",
    "    print(f\"Shape of X: {X_processed.shape}\")\n",
    "    print(f\"Number of classes in y: {len(np.unique(y))}\")\n",
    "    \n",
    "    return X_processed, y, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(model_name, random_state=42):\n",
    "    models = {\n",
    "        'logistic': LogisticRegression(max_iter=20000, random_state=random_state),\n",
    "        'lda': LDA(),\n",
    "        'qda': QDA()\n",
    "    }\n",
    "    \n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Invalid model name. Choose from: {list(models.keys())}\")\n",
    "    \n",
    "    return models[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularized_model(model_name, reg_type='ridge', C=1.0, random_state=42):\n",
    "    \"\"\"Models with regularization applied\"\"\"\n",
    "    if reg_type not in ['ridge', 'lasso']:\n",
    "        raise ValueError(\"reg_type must be 'ridge' or 'lasso'\")\n",
    "        \n",
    "    # For logistic regression, apply regularization directly\n",
    "    if model_name == 'logistic':\n",
    "        if reg_type == 'ridge':\n",
    "            return LogisticRegression(penalty='l2', C=C, solver='lbfgs', \n",
    "                                    max_iter=20000, random_state=random_state)\n",
    "        else:  # lasso\n",
    "            return LogisticRegression(penalty='l1', C=C, solver='liblinear', \n",
    "                                    max_iter=20000, random_state=random_state)\n",
    "    \n",
    "    # For LDA, map C to shrinkage parameter between 0 and 1\n",
    "    elif model_name == 'lda':\n",
    "        # Convert C to shrinkage: smaller C = stronger regularization\n",
    "        shrinkage = 1 / (1 + C)  # This will always be between 0 and 1\n",
    "        return LDA(solver='lsqr', shrinkage=shrinkage)\n",
    "    \n",
    "    # For QDA, use reg_param\n",
    "    elif model_name == 'qda':\n",
    "        # Convert C to reg_param: smaller C = stronger regularization\n",
    "        reg_param = 1 / (1 + C)  # This will always be between 0 and 1\n",
    "        return QDA(reg_param=reg_param)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name. Choose from: logistic, lda, qda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, X, y, model, random_state=42):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def calculate_f1(self, y_true, y_pred):\n",
    "        return f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    def holdout(self, test_size=0.2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        return {\n",
    "            'f1_score': self.calculate_f1(y_test, y_pred),\n",
    "            'classification_report': classification_report(y_test, y_pred),\n",
    "            'test_size': test_size\n",
    "        }\n",
    "    \n",
    "    def cross_validation(self, k):\n",
    "        scores = cross_val_score(self.model, self.X, self.y, \n",
    "                               cv=k, scoring='f1_macro')\n",
    "        return {\n",
    "            'mean_f1': np.mean(scores),\n",
    "            'std_f1': np.std(scores),\n",
    "            'all_scores': scores,\n",
    "            'k_folds': k\n",
    "        }\n",
    "    \n",
    "    def loocv(self):\n",
    "        loo = LeaveOneOut()\n",
    "        scores = cross_val_score(self.model, self.X, self.y, \n",
    "                               cv=loo, scoring='f1_macro')\n",
    "        return {\n",
    "            'mean_f1': np.mean(scores),\n",
    "            'std_f1': np.std(scores)\n",
    "        }\n",
    "    \n",
    "    def bootstrap(self, n_iterations=100, sample_size=0.8):\n",
    "        scores = []\n",
    "        n_samples = int(len(self.X) * sample_size)\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            X_boot, y_boot = resample(self.X, self.y, \n",
    "                                    n_samples=n_samples,\n",
    "                                    random_state=self.random_state + i)\n",
    "            \n",
    "            # Get out-of-bag samples using index difference\n",
    "            X_oob = self.X.loc[~self.X.index.isin(X_boot.index)]\n",
    "            y_oob = self.y.loc[~self.y.index.isin(y_boot.index)]\n",
    "            \n",
    "            # Fit on bootstrap sample and predict on out-of-bag\n",
    "            self.model.fit(X_boot, y_boot)\n",
    "            y_pred = self.model.predict(X_oob)\n",
    "            scores.append(self.calculate_f1(y_oob, y_pred))\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        ci_lower = np.percentile(scores, 2.5)\n",
    "        ci_upper = np.percentile(scores, 97.5)\n",
    "        \n",
    "        return {\n",
    "            'mean_f1': np.mean(scores),\n",
    "            'std_f1': np.std(scores),\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'n_iterations': n_iterations\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_methods(model, X, y):\n",
    "    evaluator = ModelEvaluator(X, y, model)\n",
    "    \n",
    "    results = {\n",
    "        'holdout': evaluator.holdout(),\n",
    "        'cv_5': evaluator.cross_validation(k=5),\n",
    "        'cv_10': evaluator.cross_validation(k=10),\n",
    "        'loocv': evaluator.loocv(),\n",
    "        'bootstrap': evaluator.bootstrap()\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(results, model_name):\n",
    "    data = {\n",
    "        'Method': [\n",
    "            'Holdout',\n",
    "            'CV (k=5)',\n",
    "            'CV (k=10)',\n",
    "            'LOOCV',\n",
    "            'Bootstrap'\n",
    "        ],\n",
    "        'F1-Score': [\n",
    "            results['holdout']['f1_score'],\n",
    "            results['cv_5']['mean_f1'],\n",
    "            results['cv_10']['mean_f1'],\n",
    "            results['loocv']['mean_f1'],\n",
    "            results['bootstrap']['mean_f1']\n",
    "        ],\n",
    "        'Std Dev': [\n",
    "            None,\n",
    "            results['cv_5']['std_f1'],\n",
    "            results['cv_10']['std_f1'],\n",
    "            results['loocv']['std_f1'],\n",
    "            results['bootstrap']['std_f1']\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.round(4)\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed using onehot encoding\n",
      "Shape of X: (4860, 47)\n",
      "Number of classes in y: 3\n",
      "\n",
      "Evaluating Base Models:\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for Base LOGISTIC:\n",
      "      Method  F1-Score  Std Dev\n",
      "0    Holdout    0.9119      NaN\n",
      "1   CV (k=5)    0.9122   0.0076\n",
      "2  CV (k=10)    0.9116   0.0106\n",
      "3      LOOCV    0.9265   0.2609\n",
      "4  Bootstrap    0.9078   0.0052\n",
      "\n",
      "Classification Report (Holdout Method):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       439\n",
      "           3       0.92      0.86      0.89       323\n",
      "           4       0.81      0.89      0.85       210\n",
      "\n",
      "    accuracy                           0.93       972\n",
      "   macro avg       0.91      0.92      0.91       972\n",
      "weighted avg       0.93      0.93      0.93       972\n",
      "\n",
      "\n",
      "Evaluating with Ridge Regularization:\n",
      "--------------------------------------------------\n",
      "\n",
      "Best Ridge C for logistic: 11.2884\n",
      "\n",
      "Results for Ridge LOGISTIC:\n",
      "      Method  F1-Score  Std Dev\n",
      "0    Holdout    0.9093      NaN\n",
      "1   CV (k=5)    0.9121   0.0100\n",
      "2  CV (k=10)    0.9119   0.0123\n",
      "3      LOOCV    0.9284   0.2578\n",
      "4  Bootstrap    0.9093   0.0049\n",
      "\n",
      "Classification Report (Holdout Method):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       439\n",
      "           3       0.91      0.86      0.89       323\n",
      "           4       0.81      0.89      0.84       210\n",
      "\n",
      "    accuracy                           0.93       972\n",
      "   macro avg       0.91      0.91      0.91       972\n",
      "weighted avg       0.93      0.93      0.93       972\n",
      "\n",
      "\n",
      "Evaluating with Lasso Regularization:\n",
      "--------------------------------------------------\n",
      "\n",
      "Best Lasso C for logistic: 3792.6902\n"
     ]
    }
   ],
   "source": [
    "X, y, preprocessor = load_and_preprocess_data(encoding_method='onehot')\n",
    "\n",
    "# Step 1: Evaluate base models\n",
    "print(\"\\nEvaluating Base Models:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Logistic\n",
    "base_results = {}\n",
    "model_name = 'logistic'\n",
    "model = get_base_model(model_name)\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "base_results[model_name] = results\n",
    "print(format_results(results, f\"Base {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 2: Evaluate with Ridge regularization\n",
    "print(\"\\nEvaluating with Ridge Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "C_range = np.logspace(-4, 4, 10)\n",
    "ridge_results = {}\n",
    "best_ridge_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='ridge', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_ridge_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Ridge C for {model_name}: {best_ridge_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='ridge', \n",
    "                            C=best_ridge_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "ridge_results[model_name] = results\n",
    "print(format_results(results, f\"Ridge {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 3: Evaluate with Lasso regularization\n",
    "print(\"\\nEvaluating with Lasso Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "lasso_results = {}\n",
    "best_lasso_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='lasso', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_lasso_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Lasso C for {model_name}: {best_lasso_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='lasso', \n",
    "                            C=best_lasso_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "lasso_results[model_name] = results\n",
    "print(format_results(results, f\"Lasso {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 4: Compare Results\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "base_f1 = base_results[model_name]['cv_5']['mean_f1']\n",
    "ridge_f1 = ridge_results[model_name]['cv_5']['mean_f1']\n",
    "lasso_f1 = lasso_results[model_name]['cv_5']['mean_f1']\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': model_name.upper(),\n",
    "    'Base F1': base_f1,\n",
    "    'Ridge F1': ridge_f1,\n",
    "    'Lasso F1': lasso_f1,\n",
    "    'Ridge Improvement (%)': ((ridge_f1 - base_f1) / base_f1) * 100,\n",
    "    'Lasso Improvement (%)': ((lasso_f1 - base_f1) / base_f1) * 100\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nOverall Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, preprocessor = load_and_preprocess_data(encoding_method='onehot')\n",
    "\n",
    "# Step 1: Evaluate base models\n",
    "print(\"\\nEvaluating Base Models:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Logistic\n",
    "base_results = {}\n",
    "model_name = 'lda'\n",
    "model = get_base_model(model_name)\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "base_results[model_name] = results\n",
    "print(format_results(results, f\"Base {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 2: Evaluate with Ridge regularization\n",
    "print(\"\\nEvaluating with Ridge Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "C_range = np.logspace(-4, 4, 10)\n",
    "ridge_results = {}\n",
    "best_ridge_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='ridge', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_ridge_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Ridge C for {model_name}: {best_ridge_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='ridge', \n",
    "                            C=best_ridge_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "ridge_results[model_name] = results\n",
    "print(format_results(results, f\"Ridge {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 3: Evaluate with Lasso regularization\n",
    "print(\"\\nEvaluating with Lasso Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "lasso_results = {}\n",
    "best_lasso_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='lasso', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_lasso_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Lasso C for {model_name}: {best_lasso_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='lasso', \n",
    "                            C=best_lasso_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "lasso_results[model_name] = results\n",
    "print(format_results(results, f\"Lasso {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 4: Compare Results\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "base_f1 = base_results[model_name]['cv_5']['mean_f1']\n",
    "ridge_f1 = ridge_results[model_name]['cv_5']['mean_f1']\n",
    "lasso_f1 = lasso_results[model_name]['cv_5']['mean_f1']\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': model_name.upper(),\n",
    "    'Base F1': base_f1,\n",
    "    'Ridge F1': ridge_f1,\n",
    "    'Lasso F1': lasso_f1,\n",
    "    'Ridge Improvement (%)': ((ridge_f1 - base_f1) / base_f1) * 100,\n",
    "    'Lasso Improvement (%)': ((lasso_f1 - base_f1) / base_f1) * 100\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nOverall Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, preprocessor = load_and_preprocess_data(encoding_method='onehot')\n",
    "\n",
    "# Step 1: Evaluate base models\n",
    "print(\"\\nEvaluating Base Models:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Logistic\n",
    "base_results = {}\n",
    "model_name = 'qda'\n",
    "model = get_base_model(model_name)\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "base_results[model_name] = results\n",
    "print(format_results(results, f\"Base {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 2: Evaluate with Ridge regularization\n",
    "print(\"\\nEvaluating with Ridge Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "C_range = np.logspace(-4, 4, 10)\n",
    "ridge_results = {}\n",
    "best_ridge_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='ridge', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_ridge_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Ridge C for {model_name}: {best_ridge_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='ridge', \n",
    "                            C=best_ridge_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "ridge_results[model_name] = results\n",
    "print(format_results(results, f\"Ridge {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 3: Evaluate with Lasso regularization\n",
    "print(\"\\nEvaluating with Lasso Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "lasso_results = {}\n",
    "best_lasso_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='lasso', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_lasso_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Lasso C for {model_name}: {best_lasso_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='lasso', \n",
    "                            C=best_lasso_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "lasso_results[model_name] = results\n",
    "print(format_results(results, f\"Lasso {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 4: Compare Results\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "base_f1 = base_results[model_name]['cv_5']['mean_f1']\n",
    "ridge_f1 = ridge_results[model_name]['cv_5']['mean_f1']\n",
    "lasso_f1 = lasso_results[model_name]['cv_5']['mean_f1']\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': model_name.upper(),\n",
    "    'Base F1': base_f1,\n",
    "    'Ridge F1': ridge_f1,\n",
    "    'Lasso F1': lasso_f1,\n",
    "    'Ridge Improvement (%)': ((ridge_f1 - base_f1) / base_f1) * 100,\n",
    "    'Lasso Improvement (%)': ((lasso_f1 - base_f1) / base_f1) * 100\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nOverall Comparison:\")\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
