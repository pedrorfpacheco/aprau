{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn import clone\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path='dataset.csv', encoding_method='onehot'):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    df = df.drop('Id', axis=1)\n",
    "    df['Vegetation_Type'] = df['Vegetation_Type'].str.replace('Type_', '').astype(int)\n",
    "    \n",
    "    # Split features and target\n",
    "    y = df['Vegetation_Type']\n",
    "    X = df.drop('Vegetation_Type', axis=1)\n",
    "    \n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if encoding_method == 'onehot':\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore', \n",
    "                                drop='first'), categorical_features)\n",
    "        ])\n",
    "        \n",
    "        X_processed = preprocessor.fit_transform(X)\n",
    "        numeric_names = numeric_features.tolist()\n",
    "        categorical_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "        feature_names = numeric_names + categorical_names.tolist()\n",
    "        \n",
    "    elif encoding_method == 'label':\n",
    "        label_encoders = {}\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for cat_feature in categorical_features:\n",
    "            label_encoders[cat_feature] = LabelEncoder()\n",
    "            X_transformed[cat_feature] = label_encoders[cat_feature].fit_transform(X[cat_feature])\n",
    "        \n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('all', StandardScaler(), X_transformed.columns)\n",
    "        ])\n",
    "        \n",
    "        X_processed = preprocessor.fit_transform(X_transformed)\n",
    "        feature_names = X_transformed.columns\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"encoding_method must be either 'onehot' or 'label'\")\n",
    "    \n",
    "    X_processed = pd.DataFrame(X_processed, columns=feature_names)\n",
    "    \n",
    "    print(f\"Data processed using {encoding_method} encoding\")\n",
    "    print(f\"Shape of X: {X_processed.shape}\")\n",
    "    print(f\"Number of classes in y: {len(np.unique(y))}\")\n",
    "    \n",
    "    return X_processed, y, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(model_name, random_state=42):\n",
    "    models = {\n",
    "        'logistic': LogisticRegression(max_iter=20000, random_state=random_state),\n",
    "        'lda': LDA(),\n",
    "        'qda': QDA()\n",
    "    }\n",
    "    \n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Invalid model name. Choose from: {list(models.keys())}\")\n",
    "    \n",
    "    return models[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularized_model(model_name, reg_type='ridge', C=1.0, random_state=42):\n",
    "    \"\"\"Models with regularization applied\"\"\"\n",
    "    if reg_type not in ['ridge', 'lasso']:\n",
    "        raise ValueError(\"reg_type must be 'ridge' or 'lasso'\")\n",
    "        \n",
    "    # For logistic regression, apply regularization directly\n",
    "    if model_name == 'logistic':\n",
    "        if reg_type == 'ridge':\n",
    "            return LogisticRegression(penalty='l2', C=C, solver='lbfgs', \n",
    "                                    max_iter=20000, random_state=random_state)\n",
    "        else:  # lasso\n",
    "            return LogisticRegression(penalty='l1', C=C, solver='liblinear', \n",
    "                                    max_iter=20000, random_state=random_state)\n",
    "    \n",
    "    # For LDA, map C to shrinkage parameter between 0 and 1\n",
    "    elif model_name == 'lda':\n",
    "        # Convert C to shrinkage: smaller C = stronger regularization\n",
    "        shrinkage = 1 / (1 + C)  # This will always be between 0 and 1\n",
    "        return LDA(solver='lsqr', shrinkage=shrinkage)\n",
    "    \n",
    "    # For QDA, use reg_param\n",
    "    elif model_name == 'qda':\n",
    "        # Convert C to reg_param: smaller C = stronger regularization\n",
    "        reg_param = 1 / (1 + C)  # This will always be between 0 and 1\n",
    "        return QDA(reg_param=reg_param)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name. Choose from: logistic, lda, qda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, X, y, model, random_state=42):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def calculate_f1(self, y_true, y_pred):\n",
    "        return f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    def holdout(self, test_size=0.2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        return {\n",
    "            'f1_score': self.calculate_f1(y_test, y_pred),\n",
    "            'classification_report': classification_report(y_test, y_pred),\n",
    "            'test_size': test_size\n",
    "        }\n",
    "    \n",
    "    def cross_validation(self, k):\n",
    "        scores = cross_val_score(self.model, self.X, self.y, \n",
    "                               cv=k, scoring='f1_macro')\n",
    "        return {\n",
    "            'mean_f1': np.mean(scores),\n",
    "            'std_f1': np.std(scores),\n",
    "            'all_scores': scores,\n",
    "            'k_folds': k\n",
    "        }\n",
    "    \n",
    "    def loocv(self):\n",
    "        loo = LeaveOneOut()\n",
    "        scores = cross_val_score(self.model, self.X, self.y, \n",
    "                               cv=loo, scoring='f1_macro')\n",
    "        return {\n",
    "            'mean_f1': np.mean(scores),\n",
    "            'std_f1': np.std(scores)\n",
    "        }\n",
    "    \n",
    "    def bootstrap(self, n_iterations=100, sample_size=0.8):\n",
    "        from joblib import Parallel, delayed\n",
    "        \n",
    "        def single_bootstrap():\n",
    "            # Single bootstrap iteration\n",
    "            X_boot, y_boot = resample(self.X, self.y, \n",
    "                                    n_samples=int(len(self.X) * sample_size))\n",
    "            X_oob = self.X.loc[~self.X.index.isin(X_boot.index)]\n",
    "            y_oob = self.y.loc[~self.y.index.isin(y_boot.index)]\n",
    "            \n",
    "            model = clone(self.model)  # Create a fresh clone of the model\n",
    "            model.fit(X_boot, y_boot)\n",
    "            y_pred = model.predict(X_oob)\n",
    "            return self.calculate_f1(y_oob, y_pred)\n",
    "        \n",
    "        # Run bootstrap iterations in parallel\n",
    "        scores = Parallel(n_jobs=-1)(\n",
    "            delayed(single_bootstrap)() \n",
    "            for _ in range(n_iterations)\n",
    "        )\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        return {\n",
    "            'mean_f1': np.mean(scores),\n",
    "            'std_f1': np.std(scores),\n",
    "            'confidence_interval': (\n",
    "                np.percentile(scores, 2.5),\n",
    "                np.percentile(scores, 97.5)\n",
    "            ),\n",
    "            'n_iterations': n_iterations\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_methods(model, X, y):\n",
    "    evaluator = ModelEvaluator(X, y, model)\n",
    "    times = {}\n",
    "    results = {}\n",
    "    \n",
    "    for method in ['holdout', 'cv_5', 'cv_10', 'loocv', 'bootstrap']:\n",
    "        start_time = time.time()\n",
    "        if method == 'holdout':\n",
    "            results[method] = evaluator.holdout()\n",
    "        elif method == 'cv_5':\n",
    "            results[method] = evaluator.cross_validation(k=5)\n",
    "        elif method == 'cv_10':\n",
    "            results[method] = evaluator.cross_validation(k=10)\n",
    "        elif method == 'loocv':\n",
    "            results[method] = evaluator.loocv()\n",
    "        else:  # bootstrap\n",
    "            results[method] = evaluator.bootstrap()\n",
    "        times[method] = time.time() - start_time\n",
    "        print(f\"{method} took {times[method]:.2f} seconds\")\n",
    "    \n",
    "    return results, times\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(results, model_name):\n",
    "    data = {\n",
    "        'Method': [\n",
    "            'Holdout',\n",
    "            'CV (k=5)',\n",
    "            'CV (k=10)',\n",
    "            'LOOCV',\n",
    "            'Bootstrap'\n",
    "        ],\n",
    "        'F1-Score': [\n",
    "            results['holdout']['f1_score'],\n",
    "            results['cv_5']['mean_f1'],\n",
    "            results['cv_10']['mean_f1'],\n",
    "            results['loocv']['mean_f1'],\n",
    "            results['bootstrap']['mean_f1']\n",
    "        ],\n",
    "        'Std Dev': [\n",
    "            None,\n",
    "            results['cv_5']['std_f1'],\n",
    "            results['cv_10']['std_f1'],\n",
    "            results['loocv']['std_f1'],\n",
    "            results['bootstrap']['std_f1']\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.round(4)\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed using onehot encoding\n",
      "Shape of X: (4860, 47)\n",
      "Number of classes in y: 3\n",
      "\n",
      "Evaluating Base Models:\n",
      "--------------------------------------------------\n",
      "\n",
      "Results for Base LOGISTIC:\n",
      "      Method  F1-Score  Std Dev\n",
      "0    Holdout    0.9119      NaN\n",
      "1   CV (k=5)    0.9122   0.0076\n",
      "2  CV (k=10)    0.9116   0.0106\n",
      "3      LOOCV    0.9265   0.2609\n",
      "4  Bootstrap    0.9078   0.0052\n",
      "\n",
      "Classification Report (Holdout Method):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       439\n",
      "           3       0.92      0.86      0.89       323\n",
      "           4       0.81      0.89      0.85       210\n",
      "\n",
      "    accuracy                           0.93       972\n",
      "   macro avg       0.91      0.92      0.91       972\n",
      "weighted avg       0.93      0.93      0.93       972\n",
      "\n",
      "\n",
      "Evaluating with Ridge Regularization:\n",
      "--------------------------------------------------\n",
      "\n",
      "Best Ridge C for logistic: 2.7826\n",
      "\n",
      "Results for Ridge LOGISTIC:\n",
      "      Method  F1-Score  Std Dev\n",
      "0    Holdout    0.9081      NaN\n",
      "1   CV (k=5)    0.9115   0.0094\n",
      "2  CV (k=10)    0.9114   0.0117\n",
      "3      LOOCV    0.9282   0.2582\n",
      "4  Bootstrap    0.9092   0.0051\n",
      "\n",
      "Classification Report (Holdout Method):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       439\n",
      "           3       0.91      0.86      0.88       323\n",
      "           4       0.80      0.89      0.84       210\n",
      "\n",
      "    accuracy                           0.93       972\n",
      "   macro avg       0.91      0.91      0.91       972\n",
      "weighted avg       0.93      0.93      0.93       972\n",
      "\n",
      "\n",
      "Evaluating with Lasso Regularization:\n",
      "--------------------------------------------------\n",
      "\n",
      "Best Lasso C for logistic: 166.8101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Evaluate with best C\u001b[39;00m\n\u001b[1;32m     60\u001b[0m model \u001b[38;5;241m=\u001b[39m get_regularized_model(model_name, reg_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     61\u001b[0m                             C\u001b[38;5;241m=\u001b[39mbest_lasso_C[model_name])\n\u001b[0;32m---> 62\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_all_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m lasso_results[model_name] \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(format_results(results, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLasso \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m, in \u001b[0;36mevaluate_all_methods\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_all_methods\u001b[39m(model, X, y):\n\u001b[1;32m      2\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluator(X, y, model)\n\u001b[1;32m      4\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mholdout\u001b[39m\u001b[38;5;124m'\u001b[39m: evaluator\u001b[38;5;241m.\u001b[39mholdout(),\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_5\u001b[39m\u001b[38;5;124m'\u001b[39m: evaluator\u001b[38;5;241m.\u001b[39mcross_validation(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_10\u001b[39m\u001b[38;5;124m'\u001b[39m: evaluator\u001b[38;5;241m.\u001b[39mcross_validation(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloocv\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloocv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: evaluator\u001b[38;5;241m.\u001b[39mbootstrap()\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[39], line 37\u001b[0m, in \u001b[0;36mModelEvaluator.loocv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloocv\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m     loo \u001b[38;5;241m=\u001b[39m LeaveOneOut()\n\u001b[0;32m---> 37\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1_macro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(scores),\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mstd(scores)\n\u001b[1;32m     42\u001b[0m     }\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1276\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1273\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[1;32m   1275\u001b[0m         )\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Projects/aprau/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1215\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1212\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m   1214\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1215\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y, preprocessor = load_and_preprocess_data(encoding_method='onehot')\n",
    "\n",
    "# Step 1: Evaluate base models\n",
    "print(\"\\nEvaluating Base Models:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Logistic\n",
    "base_results = {}\n",
    "model_name = 'logistic'\n",
    "model = get_base_model(model_name)\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "base_results[model_name] = results\n",
    "print(format_results(results, f\"Base {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 2: Evaluate with Ridge regularization\n",
    "print(\"\\nEvaluating with Ridge Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "C_range = np.logspace(-4, 4, 10)\n",
    "ridge_results = {}\n",
    "best_ridge_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='ridge', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_ridge_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Ridge C for {model_name}: {best_ridge_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='ridge', \n",
    "                            C=best_ridge_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "ridge_results[model_name] = results\n",
    "print(format_results(results, f\"Ridge {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 3: Evaluate with Lasso regularization\n",
    "print(\"\\nEvaluating with Lasso Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "lasso_results = {}\n",
    "best_lasso_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='lasso', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_lasso_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Lasso C for {model_name}: {best_lasso_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='lasso', \n",
    "                            C=best_lasso_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "lasso_results[model_name] = results\n",
    "print(format_results(results, f\"Lasso {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 4: Compare Results\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "base_f1 = base_results[model_name]['cv_5']['mean_f1']\n",
    "ridge_f1 = ridge_results[model_name]['cv_5']['mean_f1']\n",
    "lasso_f1 = lasso_results[model_name]['cv_5']['mean_f1']\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': model_name.upper(),\n",
    "    'Base F1': base_f1,\n",
    "    'Ridge F1': ridge_f1,\n",
    "    'Lasso F1': lasso_f1,\n",
    "    'Ridge Improvement (%)': ((ridge_f1 - base_f1) / base_f1) * 100,\n",
    "    'Lasso Improvement (%)': ((lasso_f1 - base_f1) / base_f1) * 100\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nOverall Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, preprocessor = load_and_preprocess_data(encoding_method='onehot')\n",
    "\n",
    "# Step 1: Evaluate base models\n",
    "print(\"\\nEvaluating Base Models:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Logistic\n",
    "base_results = {}\n",
    "model_name = 'lda'\n",
    "model = get_base_model(model_name)\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "base_results[model_name] = results\n",
    "print(format_results(results, f\"Base {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 2: Evaluate with Ridge regularization\n",
    "print(\"\\nEvaluating with Ridge Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "C_range = np.logspace(-4, 4, 10)\n",
    "ridge_results = {}\n",
    "best_ridge_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='ridge', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_ridge_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Ridge C for {model_name}: {best_ridge_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='ridge', \n",
    "                            C=best_ridge_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "ridge_results[model_name] = results\n",
    "print(format_results(results, f\"Ridge {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 3: Evaluate with Lasso regularization\n",
    "print(\"\\nEvaluating with Lasso Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "lasso_results = {}\n",
    "best_lasso_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='lasso', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_lasso_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Lasso C for {model_name}: {best_lasso_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='lasso', \n",
    "                            C=best_lasso_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "lasso_results[model_name] = results\n",
    "print(format_results(results, f\"Lasso {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 4: Compare Results\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "base_f1 = base_results[model_name]['cv_5']['mean_f1']\n",
    "ridge_f1 = ridge_results[model_name]['cv_5']['mean_f1']\n",
    "lasso_f1 = lasso_results[model_name]['cv_5']['mean_f1']\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': model_name.upper(),\n",
    "    'Base F1': base_f1,\n",
    "    'Ridge F1': ridge_f1,\n",
    "    'Lasso F1': lasso_f1,\n",
    "    'Ridge Improvement (%)': ((ridge_f1 - base_f1) / base_f1) * 100,\n",
    "    'Lasso Improvement (%)': ((lasso_f1 - base_f1) / base_f1) * 100\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nOverall Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, preprocessor = load_and_preprocess_data(encoding_method='onehot')\n",
    "\n",
    "# Step 1: Evaluate base models\n",
    "print(\"\\nEvaluating Base Models:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Logistic\n",
    "base_results = {}\n",
    "model_name = 'qda'\n",
    "model = get_base_model(model_name)\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "base_results[model_name] = results\n",
    "print(format_results(results, f\"Base {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 2: Evaluate with Ridge regularization\n",
    "print(\"\\nEvaluating with Ridge Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "C_range = np.logspace(-4, 4, 10)\n",
    "ridge_results = {}\n",
    "best_ridge_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='ridge', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_ridge_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Ridge C for {model_name}: {best_ridge_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='ridge', \n",
    "                            C=best_ridge_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "ridge_results[model_name] = results\n",
    "print(format_results(results, f\"Ridge {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 3: Evaluate with Lasso regularization\n",
    "print(\"\\nEvaluating with Lasso Regularization:\")\n",
    "print(\"-\" * 50)\n",
    "lasso_results = {}\n",
    "best_lasso_C = {}\n",
    "\n",
    "# Find best C\n",
    "cv_scores = []\n",
    "for C in C_range:\n",
    "    model = get_regularized_model(model_name, reg_type='lasso', C=C)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "best_lasso_C[model_name] = C_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest Lasso C for {model_name}: {best_lasso_C[model_name]:.4f}\")\n",
    "\n",
    "# Evaluate with best C\n",
    "model = get_regularized_model(model_name, reg_type='lasso', \n",
    "                            C=best_lasso_C[model_name])\n",
    "results = evaluate_all_methods(model, X, y)\n",
    "lasso_results[model_name] = results\n",
    "print(format_results(results, f\"Lasso {model_name.upper()}\"))\n",
    "print(\"\\nClassification Report (Holdout Method):\")\n",
    "print(results['holdout']['classification_report'])\n",
    "\n",
    "# Step 4: Compare Results\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "base_f1 = base_results[model_name]['cv_5']['mean_f1']\n",
    "ridge_f1 = ridge_results[model_name]['cv_5']['mean_f1']\n",
    "lasso_f1 = lasso_results[model_name]['cv_5']['mean_f1']\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': model_name.upper(),\n",
    "    'Base F1': base_f1,\n",
    "    'Ridge F1': ridge_f1,\n",
    "    'Lasso F1': lasso_f1,\n",
    "    'Ridge Improvement (%)': ((ridge_f1 - base_f1) / base_f1) * 100,\n",
    "    'Lasso Improvement (%)': ((lasso_f1 - base_f1) / base_f1) * 100\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nOverall Comparison:\")\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
